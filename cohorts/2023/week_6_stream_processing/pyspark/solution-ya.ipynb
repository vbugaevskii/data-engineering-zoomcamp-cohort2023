{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3a57662-6e54-40ed-ac01-12d46bab534d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SPARK_HOME=/usr/lib/spark\n",
      "env: SPARK_KAFKA_VERSION=0.10\n"
     ]
    }
   ],
   "source": [
    "%env SPARK_HOME=/usr/lib/spark\n",
    "%env SPARK_KAFKA_VERSION=0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "563e79f4-38f2-48d7-a8ea-5cda05fffcee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86c6bc54-aebb-4c7a-92cb-f73f7300e5ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init('/usr/lib/spark/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc6293a-2dfd-4028-abd6-1ca1103512fd",
   "metadata": {},
   "source": [
    "## PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d92049",
   "metadata": {},
   "source": [
    "You can find the updated instructions how to run Data Proc with Spark at directory [`week_5_batch_processing`](https://github.com/vbugaevskii/data-engineering-zoomcamp-cohort2023/blob/main/cohorts/2023/week_5_batch_processing/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "144bf3f3-71a0-4e7c-9a57-f8c9af5c24fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pyspark\n",
    "\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b45343ad-7dde-47c8-9115-86ee3235d933",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;34mINFO\u001b[m] Scanning for projects...\n",
      "[\u001b[1;34mINFO\u001b[m] \n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m------------------< \u001b[0;36mcom.dataclub.zoomcamp.de:pyspark\u001b[0;1m >------------------\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1mBuilding pyspark 2.0\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m]   from pom.xml\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m--------------------------------[ jar ]---------------------------------\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] \n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m--- \u001b[0;32mdependency:2.8:copy-dependencies\u001b[m \u001b[1m(default-cli)\u001b[m @ \u001b[36mpyspark\u001b[0;1m ---\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] Copying kafka-clients-3.2.0.jar to /home/vbugaevskii/pyspark/jars/kafka-clients-3.2.0.jar\n",
      "[\u001b[1;34mINFO\u001b[m] Copying unused-1.0.0.jar to /home/vbugaevskii/pyspark/jars/unused-1.0.0.jar\n",
      "[\u001b[1;34mINFO\u001b[m] Copying lz4-java-1.8.0.jar to /home/vbugaevskii/pyspark/jars/lz4-java-1.8.0.jar\n",
      "[\u001b[1;34mINFO\u001b[m] Copying scala-library-2.12.10.jar to /home/vbugaevskii/pyspark/jars/scala-library-2.12.10.jar\n",
      "[\u001b[1;34mINFO\u001b[m] Copying snappy-java-1.1.8.4.jar to /home/vbugaevskii/pyspark/jars/snappy-java-1.1.8.4.jar\n",
      "[\u001b[1;34mINFO\u001b[m] Copying spark-sql-kafka-0-10_2.12-3.0.3.jar to /home/vbugaevskii/pyspark/jars/spark-sql-kafka-0-10_2.12-3.0.3.jar\n",
      "[\u001b[1;34mINFO\u001b[m] Copying slf4j-api-1.7.36.jar to /home/vbugaevskii/pyspark/jars/slf4j-api-1.7.36.jar\n",
      "[\u001b[1;34mINFO\u001b[m] Copying spark-token-provider-kafka-0-10_2.12-3.0.3.jar to /home/vbugaevskii/pyspark/jars/spark-token-provider-kafka-0-10_2.12-3.0.3.jar\n",
      "[\u001b[1;34mINFO\u001b[m] Copying spark-streaming-kafka-0-10_2.12-3.0.3.jar to /home/vbugaevskii/pyspark/jars/spark-streaming-kafka-0-10_2.12-3.0.3.jar\n",
      "[\u001b[1;34mINFO\u001b[m] Copying commons-pool2-2.6.2.jar to /home/vbugaevskii/pyspark/jars/commons-pool2-2.6.2.jar\n",
      "[\u001b[1;34mINFO\u001b[m] Copying spark-avro_2.12-3.0.3.jar to /home/vbugaevskii/pyspark/jars/spark-avro_2.12-3.0.3.jar\n",
      "[\u001b[1;34mINFO\u001b[m] Copying spark-tags_2.12-3.0.3.jar to /home/vbugaevskii/pyspark/jars/spark-tags_2.12-3.0.3.jar\n",
      "[\u001b[1;34mINFO\u001b[m] Copying zstd-jni-1.5.2-1.jar to /home/vbugaevskii/pyspark/jars/zstd-jni-1.5.2-1.jar\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m------------------------------------------------------------------------\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1;32mBUILD SUCCESS\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m------------------------------------------------------------------------\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] Total time:  1.633 s\n",
      "[\u001b[1;34mINFO\u001b[m] Finished at: 2023-03-12T16:44:06Z\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m------------------------------------------------------------------------\u001b[m\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "# NOTE: This works properly for pyspark==3.0.3\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "!rm -r jars || true\n",
    "!mkdir -p jars\n",
    "!mvn dependency:copy-dependencies -DoutputDirectory=jars\n",
    "\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f114e3a-fafd-43bd-8950-d420405d24a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.128.0.22:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=pyspark-shell>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: jar_packages works properly for spark==3.0.3\n",
    "\n",
    "jar_packages = [\n",
    "    \"org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.3\",\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.3\",\n",
    "    \"org.apache.spark:spark-avro_2.12:3.0.3\",\n",
    "    \"org.apache.kafka:kafka-clients:3.2.0\",\n",
    "    # \"org.apache.kafka:kafka-clients:0.10.0.1\",\n",
    "]\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .master(\"yarn\")\n",
    "        # .config(\"spark.jars\", ','.join(map(str, Path(\"jars\").glob(\"*.jar\"))))\n",
    "        .config(\"spark.jars.packages\", ','.join(jar_packages))\n",
    "        .config(\"spark.executor.cores\", 2)\n",
    "        .config(\"spark.executor.instances\", 4)\n",
    "        .config(\"spark.executor.memory\", \"2G\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0750b558-2b4f-48c8-a7b3-cd729b16d3ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_from_kafka(topic: str) -> pyspark.sql.DataFrame:\n",
    "    servers = [\n",
    "        \"rc1a-cfsongcosevdstr7.mdb.yandexcloud.net:9092\",\n",
    "        \"rc1a-dpcbjr36v0m3hqij.mdb.yandexcloud.net:9092\",\n",
    "        \"rc1a-rb5l0smprrcqojlp.mdb.yandexcloud.net:9092\",\n",
    "        \"rc1a-up0snrtao9kga6n8.mdb.yandexcloud.net:9092\",\n",
    "    ]\n",
    "\n",
    "    stream = (\n",
    "        spark.readStream\n",
    "            .format(\"kafka\")\n",
    "            .option(\"kafka.bootstrap.servers\", \",\".join(servers))\n",
    "            .option(\"kafka.sasl.mechanism\", \"SCRAM-SHA-256\")\n",
    "            .option(\"kafka.security.protocol\", \"SASL_PLAINTEXT\")\n",
    "            .option(\"kafka.sasl.jaas.config\", f\"org.apache.kafka.common.security.scram.ScramLoginModule required username=\\\"{os.environ['KAFKA_USER']}\\\" password=\\\"{os.environ['KAFKA_PASS']}\\\";\")\n",
    "            .option(\"kafka.partition.assignment.strategy\", \"range\")\n",
    "            .option(\"subscribe\", topic)\n",
    "            .option(\"startingOffsets\", \"earliest\")\n",
    "            .option(\"checkpointLocation\", \"checkpoint\")\n",
    "            .load()\n",
    "    )\n",
    "\n",
    "    return stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fb4fe90-d95f-42c0-b389-68fc46140aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_taxi_green = read_from_kafka(\"dev-topic\").selectExpr(\"CAST(value AS STRING)\")\n",
    "df_taxi_green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12ccf7dc-17ab-4556-8276-076c40bf5349",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_taxi_green.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ee16c3d-9885-4b12-86c3-613ce2fab8ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxi_green.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"rides_green_table\") \\\n",
    "    .start()\n",
    "\n",
    "spark.sql(\"select * from rides_green_table\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfff704c-69b6-49f4-9f09-70a17c598eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7ff21813aa90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_taxi_green.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a06ff12-b666-4166-8ae1-ed897c43ed1a",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "At first I have been fighting to run PySpark connected to Kafka:\n",
    "- https://spark.apache.org/docs/2.4.6/streaming-kafka-0-8-integration.html\n",
    "- https://spark.apache.org/docs/3.0.3/structured-streaming-kafka-integration.html\n",
    "\n",
    "Finnaly I have managed to run spark using `spark.jars.packages` or `maven`. The solution for with env `PYSPARK_SUBMIT_ARGS` from [the example](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_6_stream_processing/python/streams-example/pyspark/streaming-notebook.ipynb) didn't work for me. I suppose, the problem is that my virtual machine is not a part of Data Proc cluster (it is not a master machine).\n",
    "\n",
    "Then I have been fighting with [missing`partition.assignment.strategy` parameter](https://stackoverflow.com/questions/65890891/kafka-partition-assignment-strategy-in-pyspark). Finnaly, I understood that this parameter is an option for a consumer and should be placed in `readStream` operator and the option should have prefix `kafka`.\n",
    "\n",
    "Now you can see the final output of my program. The output is empty because of the error in Spark:\n",
    "\n",
    "```\n",
    "java.lang.NoSuchMethodError: 'void org.apache.kafka.clients.consumer.KafkaConsumer.subscribe(java.util.Collection)'\n",
    "```\n",
    "\n",
    "I suppose this error occures because of libraries incompatability. I have spent all my weekend trying to solve this problem, but I have failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8983cf29-d9cb-4325-8c61-4360c24ba7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('de-zoomcamp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f6a41d74890ee68e412505c7fb98610353fa8122a4a53aacf13cd4761eb6b38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
