{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f583e5ae-022b-41c6-b708-fb8ea1b4fde3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "112c3570-65bb-405b-926a-f47d349bbb6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8457139e-b0a1-4742-8df6-62a1e94e379b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-400a85e3-93a7-46c4-a2b4-3bea90f30206;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.3.1 in central\n",
      "\tfound org.tukaani#xz;1.8 in central\n",
      ":: resolution report :: resolve 963ms :: artifacts dl 48ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.3.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.tukaani#xz;1.8 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   14  |   0   |   0   |   0   ||   14  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-400a85e3-93a7-46c4-a2b4-3bea90f30206\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 14 already retrieved (0kB/20ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 21:06:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://3905ae53d793:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark-Notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Spark-Notebook>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jar_packages = [\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1\",\n",
    "    \"org.apache.spark:spark-avro_2.12:3.3.1\",\n",
    "]\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder \\\n",
    "        .appName(\"Spark-Notebook\") \\\n",
    "        .config(\"spark.jars.packages\", ','.join(jar_packages)) \\\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1836ccf5-f482-4417-a9da-44d721d17128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_from_kafka(topic: str) -> pyspark.sql.DataFrame:\n",
    "    servers = [\n",
    "        \"localhost:9092\",\n",
    "        \"broker:29092\",\n",
    "    ]\n",
    "\n",
    "    stream = (\n",
    "        spark.readStream\n",
    "            .format(\"kafka\")\n",
    "            .option(\"kafka.bootstrap.servers\", \",\".join(servers))\n",
    "            .option(\"subscribe\", topic)\n",
    "            .option(\"startingOffsets\", \"earliest\")\n",
    "            .option(\"checkpointLocation\", \"checkpoint\")\n",
    "            .load()\n",
    "    )\n",
    "\n",
    "    return stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e089d-768a-44de-a1cc-f8334fc97fb6",
   "metadata": {},
   "source": [
    "### Process Green Taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d0dd470-eb00-4c6b-9bdd-c69b38f22758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SCHEMA_GREEN = T.StructType([\n",
    "    T.StructField('VendorID',              T.IntegerType(),   True),\n",
    "    T.StructField('lpep_pickup_datetime',  T.TimestampType(), True),\n",
    "    T.StructField('lpep_dropoff_datetime', T.TimestampType(), True),\n",
    "    T.StructField('store_and_fwd_flag',    T.StringType(),    True),\n",
    "    T.StructField('RatecodeID',            T.IntegerType(),   True),\n",
    "    T.StructField('PULocationID',          T.IntegerType(),   True),\n",
    "    T.StructField('DOLocationID',          T.IntegerType(),   True),\n",
    "    T.StructField('passenger_count',       T.IntegerType(),   True),\n",
    "    T.StructField('trip_distance',         T.FloatType(),     True),\n",
    "    T.StructField('fare_amount',           T.FloatType(),     True),\n",
    "    T.StructField('extra',                 T.FloatType(),     True),\n",
    "    T.StructField('mta_tax',               T.FloatType(),     True),\n",
    "    T.StructField('tip_amount',            T.FloatType(),     True),\n",
    "    T.StructField('tolls_amount',          T.FloatType(),     True),\n",
    "    T.StructField('ehail_fee',             T.FloatType(),     True),\n",
    "    T.StructField('improvement_surcharge', T.FloatType(),     True),\n",
    "    T.StructField('total_amount',          T.FloatType(),     True),\n",
    "    T.StructField('payment_type',          T.IntegerType(),   True),\n",
    "    T.StructField('trip_type',             T.IntegerType(),   True),\n",
    "    T.StructField('congestion_surcharge',  T.FloatType(),     True),\n",
    "])\n",
    "\n",
    "\n",
    "def parse_green_ride_from_kafka_message(df_raw):\n",
    "    assert df_raw.isStreaming is True, \"DataFrame doesn't receive streaming data\"\n",
    "\n",
    "    df = df_raw.select(F.from_json(F.col(\"value\").cast(\"string\"), SCHEMA_GREEN).alias(\"value\"))\n",
    "    \n",
    "    df = df.selectExpr(\n",
    "        'value.VendorID',\n",
    "        'value.lpep_pickup_datetime as pickup_datetime',\n",
    "        'value.lpep_dropoff_datetime as dropoff_datetime',\n",
    "        'value.PULocationID',\n",
    "        'value.DOLocationID',\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c390c078-a116-41e3-819a-2601ff1aacef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxi_green_raw = read_from_kafka(\"rides_green\")\n",
    "df_taxi_green_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaad0618-64f8-48f4-8784-b4d080f181f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxi_green = parse_green_ride_from_kafka_message(df_taxi_green_raw)\n",
    "df_taxi_green.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51d39ed1-6ff5-4627-8390-265434f4fbd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sink_console(df, output_mode: str = 'complete', processing_time: str = '5 seconds'):\n",
    "    write_query = (\n",
    "        df.writeStream\n",
    "            .outputMode(output_mode)\n",
    "            .trigger(processingTime=processing_time)\n",
    "            .format(\"console\")\n",
    "            .option(\"truncate\", False)\n",
    "            .start()\n",
    "    )\n",
    "    \n",
    "    return write_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92a629e2-9168-4795-b174-365bf991b39f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 21:07:00 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-7935ae93-a444-4ed7-a648-009d76e599fe. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/03/12 21:07:00 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f514d4b6350>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 21:07:01 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fad8243a-6bdd-4f94-8203-f4fb4dee6544-1129940546-driver-0-1, groupId=spark-kafka-source-fad8243a-6bdd-4f94-8203-f4fb4dee6544-1129940546-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/03/12 21:07:01 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-fad8243a-6bdd-4f94-8203-f4fb4dee6544-1129940546-driver-0-1, groupId=spark-kafka-source-fad8243a-6bdd-4f94-8203-f4fb4dee6544-1129940546-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------+-------------------+-------------------+------------+------------+\n",
      "|VendorID|pickup_datetime    |dropoff_datetime   |PULocationID|DOLocationID|\n",
      "+--------+-------------------+-------------------+------------+------------+\n",
      "|2       |2018-12-21 15:17:29|2018-12-21 15:18:57|264         |264         |\n",
      "|2       |2019-01-01 00:10:16|2019-01-01 00:16:32|97          |49          |\n",
      "|2       |2019-01-01 00:27:11|2019-01-01 00:31:38|49          |189         |\n",
      "|2       |2019-01-01 00:46:20|2019-01-01 01:04:54|189         |17          |\n",
      "|2       |2019-01-01 00:19:06|2019-01-01 00:39:43|82          |258         |\n",
      "|2       |2019-01-01 00:12:35|2019-01-01 00:19:09|49          |17          |\n",
      "|2       |2019-01-01 00:47:55|2019-01-01 01:00:01|255         |33          |\n",
      "|1       |2019-01-01 00:12:47|2019-01-01 00:30:50|76          |225         |\n",
      "|2       |2019-01-01 00:16:23|2019-01-01 00:39:46|25          |89          |\n",
      "|2       |2019-01-01 00:58:02|2019-01-01 01:19:02|85          |39          |\n",
      "|2       |2019-01-01 00:37:00|2019-01-01 00:56:42|223         |238         |\n",
      "|2       |2019-01-01 00:13:48|2019-01-01 00:21:00|129         |129         |\n",
      "|2       |2019-01-01 00:19:59|2019-01-01 00:45:50|71          |71          |\n",
      "|2       |2019-01-01 00:57:57|2019-01-01 01:20:10|85          |177         |\n",
      "|1       |2019-01-01 00:09:02|2019-01-01 00:17:50|256         |80          |\n",
      "|1       |2019-01-01 00:22:12|2019-01-01 00:25:29|80          |80          |\n",
      "|1       |2019-01-01 00:31:55|2019-01-01 00:52:59|256         |229         |\n",
      "|2       |2019-01-01 00:30:20|2019-01-01 00:54:19|255         |231         |\n",
      "|2       |2018-12-31 23:58:06|2019-01-01 00:00:57|146         |7           |\n",
      "|2       |2019-01-01 00:40:17|2019-01-01 00:50:23|146         |129         |\n",
      "+--------+-------------------+-------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "23/03/12 21:07:08 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 7303 milliseconds\n"
     ]
    }
   ],
   "source": [
    "sink_console(df_taxi_green, output_mode='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af791f4-81de-4ec1-ba02-0a4e9e8eeb30",
   "metadata": {},
   "source": [
    "### Process Fhv Taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cac6582-b180-41ba-a495-e245e58a85b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SCHEMA_FHV = T.StructType([\n",
    "    T.StructField('dispatching_base_num',   T.StringType(),    True),\n",
    "    T.StructField('pickup_datetime',        T.TimestampType(), True),\n",
    "    T.StructField('dropOff_datetime',       T.TimestampType(), True),\n",
    "    T.StructField('PUlocationID',           T.IntegerType(),   True),\n",
    "    T.StructField('DOlocationID',           T.IntegerType(),   True),\n",
    "    T.StructField('SR_Flag',                T.StringType(),    True),\n",
    "    T.StructField('Affiliated_base_number', T.StringType(),    True),\n",
    "])\n",
    "\n",
    "\n",
    "def parse_fhv_ride_from_kafka_message(df_raw):\n",
    "    assert df_raw.isStreaming is True, \"DataFrame doesn't receive streaming data\"\n",
    "\n",
    "    df = df_raw.select(F.from_json(F.col(\"value\").cast(\"string\"), SCHEMA_FHV).alias(\"value\"))\n",
    "    \n",
    "    df = df.selectExpr(\n",
    "        'value.dispatching_base_num as VendorID',\n",
    "        'value.pickup_datetime as pickup_datetime',\n",
    "        'value.dropoff_datetime as dropoff_datetime',\n",
    "        'value.PUlocationID as PULocationID',\n",
    "        'value.DOlocationID as DOLocationID',\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0763f3fd-3ea3-435d-873a-8513653dad99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxi_fhv_raw = read_from_kafka(\"rides_fhv\")\n",
    "df_taxi_fhv_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f82452e-4e27-4fc6-be63-0207895b06f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxi_fhv = parse_fhv_ride_from_kafka_message(df_taxi_fhv_raw)\n",
    "df_taxi_fhv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "897d02fb-cde9-4f06-b8f3-69af888d574b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 21:07:30 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-df5f310a-88cd-4143-b859-a8ec37e2308b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/03/12 21:07:30 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f514d45f910>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------+-------------------+-------------------+------------+------------+\n",
      "|VendorID|pickup_datetime    |dropoff_datetime   |PUlocationID|DOlocationID|\n",
      "+--------+-------------------+-------------------+------------+------------+\n",
      "|B00254  |2019-01-01 00:33:03|2019-01-01 01:37:24|null        |null        |\n",
      "|B00254  |2019-01-01 00:03:00|2019-01-01 00:34:25|null        |null        |\n",
      "|B00254  |2019-01-01 00:45:48|2019-01-01 01:26:01|null        |null        |\n",
      "|B00254  |2019-01-01 00:37:39|2019-01-01 01:44:59|null        |null        |\n",
      "|B00254  |2019-01-01 00:35:06|2019-01-01 01:30:21|null        |null        |\n",
      "|B00254  |2019-01-01 00:55:23|2019-01-01 01:48:27|null        |null        |\n",
      "|B00254  |2019-01-01 00:49:23|2019-01-01 01:38:38|null        |null        |\n",
      "|B00254  |2019-01-01 00:11:10|2019-01-01 00:44:31|null        |null        |\n",
      "|B00254  |2019-01-01 00:00:06|2019-01-01 00:32:21|null        |null        |\n",
      "|B00254  |2019-01-01 00:36:32|2019-01-01 01:35:54|null        |null        |\n",
      "|B00254  |2019-01-01 00:15:15|2019-01-01 00:54:49|null        |null        |\n",
      "|B00445  |2019-01-01 00:32:02|2019-01-01 00:56:51|null        |null        |\n",
      "|B00445  |2019-01-01 00:25:50|2019-01-01 00:33:58|null        |null        |\n",
      "|B00445  |2019-01-01 00:45:47|2019-01-01 01:03:04|null        |null        |\n",
      "|B00445  |2019-01-01 00:36:25|2019-01-01 01:00:17|null        |null        |\n",
      "|B00445  |2019-01-01 00:53:37|2019-01-01 01:37:09|null        |null        |\n",
      "|B00445  |2019-01-01 00:36:15|2019-01-01 00:48:39|null        |null        |\n",
      "|B00445  |2019-01-01 00:28:34|2019-01-01 00:39:40|null        |null        |\n",
      "|B00445  |2019-01-01 00:12:32|2019-01-01 00:12:36|null        |null        |\n",
      "|B00445  |2019-01-01 00:45:07|2019-01-01 01:03:57|null        |null        |\n",
      "+--------+-------------------+-------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "23/03/12 21:07:38 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 7728 milliseconds\n"
     ]
    }
   ],
   "source": [
    "sink_console(df_taxi_fhv, output_mode='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a42e27-c777-4b57-8b7d-4ba0d5db9bde",
   "metadata": {},
   "source": [
    "### Calculate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd67b470-ac4c-4119-9ca9-c04a6e211172",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, PULocationID: int, DOLocationID: int]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_taxi = df_taxi_green.union(df_taxi_fhv).filter(F.col('PULocationID').isNotNull())\n",
    "df_taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9467368f-9b9d-49a7-8bf5-65c2e13ba5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sink_memory(df, query_name, query_template):\n",
    "    write_query = (\n",
    "        df.writeStream\n",
    "            .queryName(query_name)\n",
    "            .format('memory')\n",
    "            .start()\n",
    "    )\n",
    "    \n",
    "    query_str = query_template.format(table_name=query_name)\n",
    "    query_results = spark.sql(query_str)\n",
    "\n",
    "    return write_query, query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f68c42f0-cbe4-46df-827a-ed391c22fc39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 21:20:26 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1c6624ef-f255-43b1-b67e-4f0e6cf31b78. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/03/12 21:20:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/03/12 21:20:26 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-aa04c6bb-dd54-4dfd-9906-ec4e0ca0ccec--419651977-driver-0-5, groupId=spark-kafka-source-aa04c6bb-dd54-4dfd-9906-ec4e0ca0ccec--419651977-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/03/12 21:20:26 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-aa04c6bb-dd54-4dfd-9906-ec4e0ca0ccec--419651977-driver-0-5, groupId=spark-kafka-source-aa04c6bb-dd54-4dfd-9906-ec4e0ca0ccec--419651977-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sql_query = \"\"\"\n",
    "select PUlocationID, count(*) as cnt\n",
    "from {table_name}\n",
    "group by PUlocationID\n",
    "order by cnt desc\n",
    "limit 5\n",
    "\"\"\"\n",
    "\n",
    "df_taxi_write, df_taxi_agg = sink_memory(df_taxi, \"taxi_merged\", sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d274ad7-bbaf-47a2-8526-f3f71dfa614b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_taxi_write.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0aa9d0f-8bc6-46ab-b0ea-f70a31a969ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 21:21:05 WARN TaskSetManager: Stage 3 contains a task of very large size (5687 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|PUlocationID|   cnt|\n",
      "+------------+------+\n",
      "|        null|186779|\n",
      "|          74|  8628|\n",
      "|          41|  7110|\n",
      "|          75|  6814|\n",
      "|           7|  5762|\n",
      "+------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_taxi_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23556754-3903-4ba8-a68a-f6fe0994bcfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_taxi_write.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a58b624-3c64-4d60-9611-445a321a8ae4",
   "metadata": {},
   "source": [
    "### Write to Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5508037-a6dd-4113-b7bb-67ef3693df33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_dataframe_to_kafka_sink(df):\n",
    "    return df.select(\n",
    "        F.col('PULocationID').cast('string').alias('key'),\n",
    "        F.to_json(F.struct([F.col(x) for x in df.columns])).alias(\"value\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0eedb83e-2294-4742-a6d8-41e455e845ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_taxi_write = prepare_dataframe_to_kafka_sink(df_taxi)\n",
    "df_taxi_write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7572acc0-aed9-4bf4-b830-3f90a5eb0125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sink_kafka(df, topic, output_mode='append'):\n",
    "    servers = [\n",
    "        \"localhost:9092\",\n",
    "        \"broker:29092\",\n",
    "    ]\n",
    "    \n",
    "    write_query = (\n",
    "        df.writeStream\n",
    "            .format(\"kafka\")\n",
    "            .option(\"kafka.bootstrap.servers\", \",\".join(servers))\n",
    "            .outputMode(output_mode)\n",
    "            .option(\"topic\", topic)\n",
    "            .option(\"checkpointLocation\", \"checkpoint\")\n",
    "            .start()\n",
    "    )\n",
    "\n",
    "    return write_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad22a8a7-e282-4e0f-b5bd-25b40a371c0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 21:43:23 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f514c4b8be0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 21:43:24 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-04cf63c5-fb37-4398-b8f3-add7336acf19-1721993562-executor-11, groupId=spark-kafka-source-04cf63c5-fb37-4398-b8f3-add7336acf19-1721993562-executor] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "23/03/12 21:43:24 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-04cf63c5-fb37-4398-b8f3-add7336acf19-1721993562-executor-11, groupId=spark-kafka-source-04cf63c5-fb37-4398-b8f3-add7336acf19-1721993562-executor] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n",
      "23/03/12 21:43:24 WARN NetworkClient: [Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {rides_all=LEADER_NOT_AVAILABLE}\n",
      "23/03/12 21:43:24 WARN NetworkClient: [Producer clientId=producer-1] Error while fetching metadata with correlation id 3 : {rides_all=LEADER_NOT_AVAILABLE}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sink_kafka(df_taxi_write, 'rides_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5702969b",
   "metadata": {},
   "source": [
    "```bash\n",
    "> kcat -C -b localhost:9092 -t  rides_all -q | head\n",
    "\n",
    "{\"VendorID\":\"B00254\",\"pickup_datetime\":\"2019-01-01T00:33:03.000Z\",\"dropoff_datetime\":\"2019-01-01T01:37:24.000Z\"}\n",
    "{\"VendorID\":\"2\",\"pickup_datetime\":\"2018-12-21T15:17:29.000Z\",\"dropoff_datetime\":\"2018-12-21T15:18:57.000Z\",\"PULocationID\":264,\"DOLocationID\":264}\n",
    "{\"VendorID\":\"2\",\"pickup_datetime\":\"2019-01-01T00:10:16.000Z\",\"dropoff_datetime\":\"2019-01-01T00:16:32.000Z\",\"PULocationID\":97,\"DOLocationID\":49}\n",
    "{\"VendorID\":\"B00254\",\"pickup_datetime\":\"2019-01-01T00:03:00.000Z\",\"dropoff_datetime\":\"2019-01-01T00:34:25.000Z\"}\n",
    "{\"VendorID\":\"2\",\"pickup_datetime\":\"2019-01-01T00:27:11.000Z\",\"dropoff_datetime\":\"2019-01-01T00:31:38.000Z\",\"PULocationID\":49,\"DOLocationID\":189}\n",
    "{\"VendorID\":\"B00254\",\"pickup_datetime\":\"2019-01-01T00:45:48.000Z\",\"dropoff_datetime\":\"2019-01-01T01:26:01.000Z\"}\n",
    "{\"VendorID\":\"2\",\"pickup_datetime\":\"2019-01-01T00:46:20.000Z\",\"dropoff_datetime\":\"2019-01-01T01:04:54.000Z\",\"PULocationID\":189,\"DOLocationID\":17}\n",
    "{\"VendorID\":\"B00254\",\"pickup_datetime\":\"2019-01-01T00:37:39.000Z\",\"dropoff_datetime\":\"2019-01-01T01:44:59.000Z\"}\n",
    "{\"VendorID\":\"2\",\"pickup_datetime\":\"2019-01-01T00:19:06.000Z\",\"dropoff_datetime\":\"2019-01-01T00:39:43.000Z\",\"PULocationID\":82,\"DOLocationID\":258}\n",
    "{\"VendorID\":\"B00254\",\"pickup_datetime\":\"2019-01-01T00:35:06.000Z\",\"dropoff_datetime\":\"2019-01-01T01:30:21.000Z\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05fd395-0198-4af2-8aca-3a987859c3f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
