{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72a1d44f-23f4-4e0b-a609-ad249ca0b866",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SPARK_HOME=/usr/lib/spark\n",
      "env: SPARK_KAFKA_VERSION=0.10\n"
     ]
    }
   ],
   "source": [
    "%env SPARK_HOME=/usr/lib/spark\n",
    "%env SPARK_KAFKA_VERSION=0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ac9d486-2e94-4ab8-bbe8-e1144d51922e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2740699b-78a8-4c8f-b1fa-d8ebd43d5307",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init('/usr/lib/spark/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c62cae8e-218e-453c-9c4d-4d2a18a9c2ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 00:54:48 Hello world!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stderr,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(message)s\",\n",
    "    datefmt=\"%Y-%m-%y %H:%M:%S\",\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"py4j\")\n",
    "logger.info(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a00b143-084c-4ee2-a9f5-6cfe992ad643",
   "metadata": {},
   "source": [
    "## PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da3f289-36e2-4620-8e75-3133a7c3bdf5",
   "metadata": {},
   "source": [
    "You can find the updated instructions how to run Data Proc with Spark at directory [`week_5_batch_processing`](https://github.com/vbugaevskii/data-engineering-zoomcamp-cohort2023/blob/main/cohorts/2023/week_5_batch_processing/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f583e5ae-022b-41c6-b708-fb8ea1b4fde3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pyspark\n",
    "\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "112c3570-65bb-405b-926a-f47d349bbb6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.3'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb43f383-f18e-48d4-867e-49540a88d9cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;34mINFO\u001b[m] Scanning for projects...\n",
      "[\u001b[1;34mINFO\u001b[m] \n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m------------------< \u001b[0;36mcom.dataclub.zoomcamp.de:pyspark\u001b[0;1m >------------------\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1mBuilding pyspark 2.0\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m]   from pom.xml\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m--------------------------------[ jar ]---------------------------------\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] \n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m--- \u001b[0;32mdependency:2.8:copy-dependencies\u001b[m \u001b[1m(default-cli)\u001b[m @ \u001b[36mpyspark\u001b[0;1m ---\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] Copying spark-sql-kafka-0-10_2.12-3.0.3.jar to /home/vbugaevskii/pyspark/jars/spark-sql-kafka-0-10_2.12-3.0.3.jar\n",
      "[\u001b[1;34mINFO\u001b[m] Copying unused-1.0.0.jar to /home/vbugaevskii/pyspark/jars/unused-1.0.0.jar\n",
      "[\u001b[1;34mINFO\u001b[m] Copying spark-token-provider-kafka-0-10_2.12-3.0.3.jar to /home/vbugaevskii/pyspark/jars/spark-token-provider-kafka-0-10_2.12-3.0.3.jar\n",
      "[\u001b[1;34mINFO\u001b[m] Copying kafka-clients-2.4.1.jar to /home/vbugaevskii/pyspark/jars/kafka-clients-2.4.1.jar\n",
      "[\u001b[1;34mINFO\u001b[m] Copying zstd-jni-1.4.3-1.jar to /home/vbugaevskii/pyspark/jars/zstd-jni-1.4.3-1.jar\n",
      "[\u001b[1;34mINFO\u001b[m] Copying slf4j-api-1.7.28.jar to /home/vbugaevskii/pyspark/jars/slf4j-api-1.7.28.jar\n",
      "[\u001b[1;34mINFO\u001b[m] Copying commons-pool2-2.6.2.jar to /home/vbugaevskii/pyspark/jars/commons-pool2-2.6.2.jar\n",
      "[\u001b[1;34mINFO\u001b[m] Copying snappy-java-1.1.7.3.jar to /home/vbugaevskii/pyspark/jars/snappy-java-1.1.7.3.jar\n",
      "[\u001b[1;34mINFO\u001b[m] Copying spark-tags_2.12-3.0.3.jar to /home/vbugaevskii/pyspark/jars/spark-tags_2.12-3.0.3.jar\n",
      "[\u001b[1;34mINFO\u001b[m] Copying lz4-java-1.6.0.jar to /home/vbugaevskii/pyspark/jars/lz4-java-1.6.0.jar\n",
      "[\u001b[1;34mINFO\u001b[m] Copying scala-library-2.12.10.jar to /home/vbugaevskii/pyspark/jars/scala-library-2.12.10.jar\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m------------------------------------------------------------------------\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1;32mBUILD SUCCESS\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m------------------------------------------------------------------------\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] Total time:  1.528 s\n",
      "[\u001b[1;34mINFO\u001b[m] Finished at: 2023-03-18T00:54:53Z\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m------------------------------------------------------------------------\u001b[m\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "# NOTE: This works properly for pyspark==3.0.3\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "!rm -r jars || true\n",
    "!mkdir -p jars\n",
    "!./apache-maven-3.9.0/bin/mvn dependency:copy-dependencies -DoutputDirectory=jars\n",
    "\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdb3f8f-5145-4d95-8614-4f6a7f963fa6",
   "metadata": {},
   "source": [
    "#### Logging PySpark\n",
    "\n",
    "You can see there is little output in Jupyter Notebook. The reason is that all output is printed to console. Unfrotunately, I failed to redirect console output to jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8457139e-b0a1-4742-8df6-62a1e94e379b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.128.0.8:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=pyspark-shell>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: jar_packages works properly for spark==3.0.3\n",
    "\n",
    "jar_packages = [\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.3\",\n",
    "]\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        # .master(\"yarn\")\n",
    "        # .config(\"spark.jars\", ','.join(map(str, Path(\"jars\").glob(\"*.jar\"))))\n",
    "        .config(\"spark.jars.packages\", ','.join(jar_packages))\n",
    "        .config(\"spark.executor.cores\", 2)\n",
    "        .config(\"spark.executor.instances\", 4)\n",
    "        .config(\"spark.executor.memory\", \"2G\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1836ccf5-f482-4417-a9da-44d721d17128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_from_kafka(topic: str) -> pyspark.sql.DataFrame:\n",
    "    servers = [\n",
    "        \"rc1a-q38mpgujip0pbjir.mdb.yandexcloud.net:9092\",\n",
    "    ]\n",
    "\n",
    "    stream = (\n",
    "        spark.readStream\n",
    "            .format(\"kafka\")\n",
    "            .option(\"kafka.bootstrap.servers\", \",\".join(servers))\n",
    "            .option(\"kafka.sasl.mechanism\", \"SCRAM-SHA-512\")\n",
    "            .option(\"kafka.security.protocol\", \"SASL_PLAINTEXT\")\n",
    "            .option(\"kafka.sasl.jaas.config\", f\"org.apache.kafka.common.security.scram.ScramLoginModule required username=\\\"{os.environ['KAFKA_USER']}\\\" password=\\\"{os.environ['KAFKA_PASS']}\\\";\")\n",
    "            # .option(\"kafka.partition.assignment.strategy\", \"roundrobin\")\n",
    "            .option(\"subscribe\", topic)\n",
    "            .option(\"startingOffsets\", \"earliest\")\n",
    "            .option(\"checkpointLocation\", \"checkpoint\")\n",
    "            .load()\n",
    "    )\n",
    "\n",
    "    return stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e089d-768a-44de-a1cc-f8334fc97fb6",
   "metadata": {},
   "source": [
    "### Process Green Taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d0dd470-eb00-4c6b-9bdd-c69b38f22758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SCHEMA_GREEN = T.StructType([\n",
    "    T.StructField('VendorID',              T.IntegerType(),   True),\n",
    "    T.StructField('lpep_pickup_datetime',  T.TimestampType(), True),\n",
    "    T.StructField('lpep_dropoff_datetime', T.TimestampType(), True),\n",
    "    T.StructField('store_and_fwd_flag',    T.StringType(),    True),\n",
    "    T.StructField('RatecodeID',            T.IntegerType(),   True),\n",
    "    T.StructField('PULocationID',          T.IntegerType(),   True),\n",
    "    T.StructField('DOLocationID',          T.IntegerType(),   True),\n",
    "    T.StructField('passenger_count',       T.IntegerType(),   True),\n",
    "    T.StructField('trip_distance',         T.FloatType(),     True),\n",
    "    T.StructField('fare_amount',           T.FloatType(),     True),\n",
    "    T.StructField('extra',                 T.FloatType(),     True),\n",
    "    T.StructField('mta_tax',               T.FloatType(),     True),\n",
    "    T.StructField('tip_amount',            T.FloatType(),     True),\n",
    "    T.StructField('tolls_amount',          T.FloatType(),     True),\n",
    "    T.StructField('ehail_fee',             T.FloatType(),     True),\n",
    "    T.StructField('improvement_surcharge', T.FloatType(),     True),\n",
    "    T.StructField('total_amount',          T.FloatType(),     True),\n",
    "    T.StructField('payment_type',          T.IntegerType(),   True),\n",
    "    T.StructField('trip_type',             T.IntegerType(),   True),\n",
    "    T.StructField('congestion_surcharge',  T.FloatType(),     True),\n",
    "])\n",
    "\n",
    "\n",
    "def parse_green_ride_from_kafka_message(df_raw):\n",
    "    assert df_raw.isStreaming is True, \"DataFrame doesn't receive streaming data\"\n",
    "\n",
    "    df = df_raw.select(F.from_json(F.col(\"value\").cast(\"string\"), SCHEMA_GREEN).alias(\"value\"))\n",
    "    \n",
    "    df = df.selectExpr(\n",
    "        'value.VendorID',\n",
    "        'value.lpep_pickup_datetime as pickup_datetime',\n",
    "        'value.lpep_dropoff_datetime as dropoff_datetime',\n",
    "        'value.PULocationID',\n",
    "        'value.DOLocationID',\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c390c078-a116-41e3-819a-2601ff1aacef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxi_green_raw = read_from_kafka(\"rides_green\")\n",
    "df_taxi_green_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaad0618-64f8-48f4-8784-b4d080f181f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxi_green = parse_green_ride_from_kafka_message(df_taxi_green_raw)\n",
    "df_taxi_green.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51d39ed1-6ff5-4627-8390-265434f4fbd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sink_console(df, output_mode: str = 'complete', processing_time: str = '5 seconds'):\n",
    "    write_query = (\n",
    "        df.writeStream\n",
    "            .outputMode(output_mode)\n",
    "            .trigger(processingTime=processing_time)\n",
    "            .format(\"console\")\n",
    "            .option(\"truncate\", False)\n",
    "            .start()\n",
    "    )\n",
    "    \n",
    "    return write_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92a629e2-9168-4795-b174-365bf991b39f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fda746aeeb0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sink_console(df_taxi_green, output_mode='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af791f4-81de-4ec1-ba02-0a4e9e8eeb30",
   "metadata": {},
   "source": [
    "### Process Fhv Taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cac6582-b180-41ba-a495-e245e58a85b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SCHEMA_FHV = T.StructType([\n",
    "    T.StructField('dispatching_base_num',   T.StringType(),    True),\n",
    "    T.StructField('pickup_datetime',        T.TimestampType(), True),\n",
    "    T.StructField('dropOff_datetime',       T.TimestampType(), True),\n",
    "    T.StructField('PUlocationID',           T.IntegerType(),   True),\n",
    "    T.StructField('DOlocationID',           T.IntegerType(),   True),\n",
    "    T.StructField('SR_Flag',                T.StringType(),    True),\n",
    "    T.StructField('Affiliated_base_number', T.StringType(),    True),\n",
    "])\n",
    "\n",
    "\n",
    "def parse_fhv_ride_from_kafka_message(df_raw):\n",
    "    assert df_raw.isStreaming is True, \"DataFrame doesn't receive streaming data\"\n",
    "\n",
    "    df = df_raw.select(F.from_json(F.col(\"value\").cast(\"string\"), SCHEMA_FHV).alias(\"value\"))\n",
    "    \n",
    "    df = df.selectExpr(\n",
    "        'value.dispatching_base_num as VendorID',\n",
    "        'value.pickup_datetime as pickup_datetime',\n",
    "        'value.dropoff_datetime as dropoff_datetime',\n",
    "        'value.PUlocationID as PULocationID',\n",
    "        'value.DOlocationID as DOLocationID',\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0763f3fd-3ea3-435d-873a-8513653dad99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxi_fhv_raw = read_from_kafka(\"rides_fhv\")\n",
    "df_taxi_fhv_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f82452e-4e27-4fc6-be63-0207895b06f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxi_fhv = parse_fhv_ride_from_kafka_message(df_taxi_fhv_raw)\n",
    "df_taxi_fhv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "897d02fb-cde9-4f06-b8f3-69af888d574b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fda74102ee0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sink_console(df_taxi_fhv, output_mode='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a42e27-c777-4b57-8b7d-4ba0d5db9bde",
   "metadata": {},
   "source": [
    "### Calculate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd67b470-ac4c-4119-9ca9-c04a6e211172",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, PULocationID: int, DOLocationID: int]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_taxi = df_taxi_green.union(df_taxi_fhv).filter(F.col('PULocationID').isNotNull())\n",
    "df_taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9467368f-9b9d-49a7-8bf5-65c2e13ba5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sink_memory(df, query_name, query_template):\n",
    "    write_query = (\n",
    "        df.writeStream\n",
    "            .queryName(query_name)\n",
    "            .format('memory')\n",
    "            .start()\n",
    "    )\n",
    "    \n",
    "    query_str = query_template.format(table_name=query_name)\n",
    "    query_results = spark.sql(query_str)\n",
    "\n",
    "    return write_query, query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f68c42f0-cbe4-46df-827a-ed391c22fc39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "select PUlocationID, count(*) as cnt\n",
    "from {table_name}\n",
    "group by PUlocationID\n",
    "order by cnt desc\n",
    "limit 5\n",
    "\"\"\"\n",
    "\n",
    "df_taxi_write, df_taxi_agg = sink_memory(df_taxi, \"taxi_merged\", sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d274ad7-bbaf-47a2-8526-f3f71dfa614b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Processing new data',\n",
       " 'isDataAvailable': True,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_taxi_write.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0aa9d0f-8bc6-46ab-b0ea-f70a31a969ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+\n",
      "|PUlocationID| cnt|\n",
      "+------------+----+\n",
      "|          74|9903|\n",
      "|          75|8215|\n",
      "|          41|7937|\n",
      "|           7|6343|\n",
      "|          82|5596|\n",
      "+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxi_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23556754-3903-4ba8-a68a-f6fe0994bcfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_taxi_write.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a58b624-3c64-4d60-9611-445a321a8ae4",
   "metadata": {},
   "source": [
    "### Write to Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5508037-a6dd-4113-b7bb-67ef3693df33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_dataframe_to_kafka_sink(df):\n",
    "    return df.select(\n",
    "        F.col('PULocationID').cast('string').alias('key'),\n",
    "        F.to_json(F.struct([F.col(x) for x in df.columns])).alias(\"value\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0eedb83e-2294-4742-a6d8-41e455e845ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_taxi_write = prepare_dataframe_to_kafka_sink(df_taxi)\n",
    "df_taxi_write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7572acc0-aed9-4bf4-b830-3f90a5eb0125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sink_kafka(df, topic, output_mode='append'):\n",
    "    servers = [\n",
    "        \"rc1a-q38mpgujip0pbjir.mdb.yandexcloud.net:9092\",\n",
    "    ]\n",
    "    \n",
    "    write_query = (\n",
    "        df.writeStream\n",
    "            .format(\"kafka\")\n",
    "            .option(\"kafka.bootstrap.servers\", \",\".join(servers))\n",
    "            .outputMode(output_mode)\n",
    "            .option(\"topic\", topic)\n",
    "            .option(\"checkpointLocation\", \"checkpoint\")\n",
    "            .start()\n",
    "    )\n",
    "\n",
    "    return write_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad22a8a7-e282-4e0f-b5bd-25b40a371c0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fda74132700>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sink_kafka(df_taxi_write, 'rides_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3e2b6f-426d-43db-92e2-964b25082c63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
